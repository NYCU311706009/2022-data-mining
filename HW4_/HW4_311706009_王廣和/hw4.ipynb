{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2d8f48",
   "metadata": {},
   "source": [
    "# 作業三\n",
    "## 1.資料前處理 data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377c443",
   "metadata": {},
   "source": [
    "### a. 讀取csv前10000筆，保留text與score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e670c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5218f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24ee25fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947012a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = df[['Text','Score']][:10000]\n",
    "len(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76673a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score\n",
       "0  I have bought several of the Vitality canned d...      5\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...      1\n",
       "2  This is a confection that has been around a fe...      4\n",
       "3  If you are looking for the secret ingredient i...      2\n",
       "4  Great taffy at a great price.  There was a wid...      5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60080441",
   "metadata": {},
   "source": [
    "### 將 \"Score\" 欄位內值大於等於4的轉成1(positive)，其餘轉成0(negative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d622e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data['Score'] = sample_data['Score'].map(lambda x: 1 if x>=4 else 0) #此 map是pandas map不是一般python map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb13257d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score\n",
       "0  I have bought several of the Vitality canned d...      1\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...      0\n",
       "2  This is a confection that has been around a fe...      1\n",
       "3  If you are looking for the secret ingredient i...      0\n",
       "4  Great taffy at a great price.  There was a wid...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4c7c9",
   "metadata": {},
   "source": [
    "stop words處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fba83be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# python -m nltk.downloader all\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee54aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lower = [word.lower() for word in tokens]\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
    "    clean_text = ' '.join(lemm_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff883818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "      <td>product arrived labeled jumbo salted peanut pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "      <td>confection around century light pillowy citrus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>we switched from the advance similac to the or...</td>\n",
       "      <td>0</td>\n",
       "      <td>switched advance similac organic product think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Like the bad reviews say, the organic formula ...</td>\n",
       "      <td>1</td>\n",
       "      <td>like bad review say organic formula constipate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>I wanted to solely breastfeed but was unable t...</td>\n",
       "      <td>1</td>\n",
       "      <td>wanted solely breastfeed unable keep supplemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>i love the fact that i can get this delieved t...</td>\n",
       "      <td>1</td>\n",
       "      <td>love fact get delieved house delievy hard find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>We have a 7 week old... He had gas and constip...</td>\n",
       "      <td>1</td>\n",
       "      <td>week old gas constipation problem first week t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Score  \\\n",
       "0     I have bought several of the Vitality canned d...      1   \n",
       "1     Product arrived labeled as Jumbo Salted Peanut...      0   \n",
       "2     This is a confection that has been around a fe...      1   \n",
       "3     If you are looking for the secret ingredient i...      0   \n",
       "4     Great taffy at a great price.  There was a wid...      1   \n",
       "...                                                 ...    ...   \n",
       "9995  we switched from the advance similac to the or...      0   \n",
       "9996  Like the bad reviews say, the organic formula ...      1   \n",
       "9997  I wanted to solely breastfeed but was unable t...      1   \n",
       "9998  i love the fact that i can get this delieved t...      1   \n",
       "9999  We have a 7 week old... He had gas and constip...      1   \n",
       "\n",
       "                                                  clean  \n",
       "0     bought several vitality canned dog food produc...  \n",
       "1     product arrived labeled jumbo salted peanut pe...  \n",
       "2     confection around century light pillowy citrus...  \n",
       "3     looking secret ingredient robitussin believe f...  \n",
       "4     great taffy great price wide assortment yummy ...  \n",
       "...                                                 ...  \n",
       "9995  switched advance similac organic product think...  \n",
       "9996  like bad review say organic formula constipate...  \n",
       "9997  wanted solely breastfeed unable keep supplemen...  \n",
       "9998  love fact get delieved house delievy hard find...  \n",
       "9999  week old gas constipation problem first week t...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data['clean'] = sample_data['Text'].map(clean)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dc89654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb19aeb",
   "metadata": {},
   "source": [
    "切割資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e84a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d9bf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation = train_test_split(sample_data, test_size=0.2)\n",
    "X_train = X_train.reset_index()\n",
    "X_validation = X_validation.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f099fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X_train.drop(['Score'],axis = 1)\n",
    "train_y = X_train['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd64e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = X_validation.drop(['Score'],axis = 1)\n",
    "test_y = X_validation['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4fc505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9599"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d763c9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I did a lot of research on this fondant and really wanted to like it however it just didn\\'t work for me. When it arrived it was hard as a rock, but there is a card inside that indicates about 30-50 seconds in the microwave will make it easier to work with. This really did work well. It made it much more pliable and easier to work with. The fondant took color real well and rolled out nicely. It does say \"no shortening or powdered sugar needed\" that\\'s pretty false. If you\\'re coloring it you do need some while needing otherwise it will stick to your hands and your work surface. I DEFINITELY needed powdered sugar while rolling it out. Otherwise it stuck to the silicon mat I use. Also, after coloring and putting it in the microwave it definitely needed some time in the refrigerator before it could be rolled again and placed on the cake. Otherwise it was far too gooey.<br /><br />Finally, the moment of truth arrived, laying the rolled out fondant on the cake. It was much more difficult to work with than I imagined. It\\'s very picky on the thickness. Too thick it\\'s impossible to lay flat, too thin and it would stretch and get holes. I will say it did get some \\'elephant skin\\' during application I was able to smooth it out using a smoothing tool and the final product looked fine. Perhaps I was a little bit rusty, I used to teach a Wilton cake decorating class and have taught fondant decorating for many years but haven\\'t done a cake in the past 5-6 years. I just never remember the wilton brand, which tastes horrible, ever being this hard to work with.<br /><br />The flavor was fine on it\\'s own, but as most fondants, it\\'s incredibly sweet..incredibly. Despite the improved flavor people were still pealing it off the cake and eating the cake and frosting.<br /><br />In the end the cake looked wonderful and people were pleased but not sure I\\'ll be using this brand again.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0][1] #原始文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5621bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0][2] #label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1c12604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lot research fondant really wanted like however work arrived hard rock card inside indicates second microwave make easier work really work well made much pliable easier work fondant took color real well rolled nicely say shortening powdered sugar needed pretty false coloring need needing otherwise stick hand work surface definitely needed powdered sugar rolling otherwise stuck silicon mat use also coloring putting microwave definitely needed time refrigerator could rolled placed cake otherwise far br br finally moment truth arrived laying rolled fondant cake much difficult work imagined picky thickness thick impossible lay flat thin would stretch get hole say get skin application able smooth using smoothing tool final product looked fine perhaps little bit rusty used teach wilton cake decorating class taught fondant decorating many year done cake past year never remember wilton brand taste horrible ever hard work br br flavor fine fondant incredibly sweet incredibly despite improved flavor people still pealing cake eating cake br br end cake looked wonderful people pleased sure using brand'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0][3] #stop words清掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0cc8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取所有資料\n",
    "#X_train_text = []\n",
    "X_train_clean = []\n",
    "#X_validation_text = []\n",
    "X_validation_clean = []\n",
    "for i in range(len(X_train)):\n",
    "    #X_train_text += [X_train.iloc[i,1]]\n",
    "    X_train_clean += [X_train.iloc[i,3]]\n",
    "for i in range(len(X_validation)):\n",
    "    #X_validation_text += [X_validation.iloc[i,1]]\n",
    "    X_validation_clean += [X_validation.iloc[i,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07708977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Token\n",
    "token = Tokenizer(num_words = 4000) \n",
    "token.fit_on_texts(X_train_clean)\n",
    "token.word_index\n",
    "# 文字轉數字(token)\n",
    "X_train_seq = token.texts_to_sequences(X_train_clean)\n",
    "X_test_seq = token.texts_to_sequences(X_validation_clean)\n",
    "# 截長補短\n",
    "X_train = pad_sequences(X_train_seq, maxlen = 100) #我不知道最大長度設多少，書上寫100\n",
    "X_test = pad_sequences(X_test_seq, maxlen = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "142a1b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  20, 1852,  992,  712,  116,  466,   17, 1504,   45, 1798, 1231,\n",
       "        211,  659,  135,    1,    1,  474, 1465,  250, 1798,  973,  211,\n",
       "         19,  548,   71, 3677,  529, 2411,  624, 1535, 1138, 1012,  573,\n",
       "         11,   16, 1320,   53,   16,  706, 3396,  268,  194,  111, 2972,\n",
       "       2412,    8,  458,  262,  678,   22,   62,   49, 2131,  211, 3678,\n",
       "       1929,  973, 3678,   66,   47,  530,  211,  535,   47,   89,  618,\n",
       "       2131,   36,    4,  744,  100,  128,   71,    1,    1,    6,  262,\n",
       "        973, 1659,   55, 1659, 1139, 1749,    6,  138,   67,  211,  154,\n",
       "        211,    1,    1,  386,  211,  458,  181,  138,  397,  117,  111,\n",
       "         36])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aceef33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Flatten, MaxPooling1D\n",
    "from keras.layers import Conv1D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86008def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#要改成np的資料型別，才塞得進去model\n",
    "y_train = np.asarray(train_y).astype(np.float32)\n",
    "y_test = np.asarray(test_y).astype(np.float32)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dba7f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 32)           128000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 99, 32)            2080      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 49, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 49, 32)            0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 48, 32)            2080      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 24, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 24, 32)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24, 16)            528       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 384)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1155      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 133,843\n",
      "Trainable params: 133,843\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "50/50 - 3s - loss: 4.2717 - accuracy: 0.5723 - val_loss: 3.4175 - val_accuracy: 0.7700 - 3s/epoch - 51ms/step\n",
      "Epoch 2/15\n",
      "50/50 - 1s - loss: 2.9725 - accuracy: 0.5933 - val_loss: 2.6947 - val_accuracy: 0.7700 - 954ms/epoch - 19ms/step\n",
      "Epoch 3/15\n",
      "50/50 - 1s - loss: 2.8759 - accuracy: 0.5886 - val_loss: 2.5878 - val_accuracy: 0.7700 - 809ms/epoch - 16ms/step\n",
      "Epoch 4/15\n",
      "50/50 - 1s - loss: 2.7110 - accuracy: 0.5928 - val_loss: 2.4856 - val_accuracy: 0.7700 - 781ms/epoch - 16ms/step\n",
      "Epoch 5/15\n",
      "50/50 - 1s - loss: 2.6686 - accuracy: 0.5830 - val_loss: 2.0526 - val_accuracy: 0.7700 - 785ms/epoch - 16ms/step\n",
      "Epoch 6/15\n",
      "50/50 - 1s - loss: 2.2217 - accuracy: 0.5930 - val_loss: 1.6235 - val_accuracy: 0.7700 - 790ms/epoch - 16ms/step\n",
      "Epoch 7/15\n",
      "50/50 - 1s - loss: 2.0014 - accuracy: 0.5992 - val_loss: 1.4346 - val_accuracy: 0.7700 - 701ms/epoch - 14ms/step\n",
      "Epoch 8/15\n",
      "50/50 - 1s - loss: 1.9270 - accuracy: 0.5950 - val_loss: 1.3721 - val_accuracy: 0.7719 - 676ms/epoch - 14ms/step\n",
      "Epoch 9/15\n",
      "50/50 - 1s - loss: 1.8966 - accuracy: 0.5892 - val_loss: 1.4780 - val_accuracy: 0.7731 - 756ms/epoch - 15ms/step\n",
      "Epoch 10/15\n",
      "50/50 - 1s - loss: 1.8152 - accuracy: 0.5878 - val_loss: 1.4055 - val_accuracy: 0.7769 - 787ms/epoch - 16ms/step\n",
      "Epoch 11/15\n",
      "50/50 - 1s - loss: 1.7911 - accuracy: 0.5828 - val_loss: 1.9292 - val_accuracy: 0.7738 - 772ms/epoch - 15ms/step\n",
      "Epoch 12/15\n",
      "50/50 - 1s - loss: 1.7251 - accuracy: 0.6011 - val_loss: 1.4052 - val_accuracy: 0.7738 - 733ms/epoch - 15ms/step\n",
      "Epoch 13/15\n",
      "50/50 - 1s - loss: 1.6952 - accuracy: 0.5938 - val_loss: 1.3002 - val_accuracy: 0.7775 - 766ms/epoch - 15ms/step\n",
      "Epoch 14/15\n",
      "50/50 - 1s - loss: 1.7092 - accuracy: 0.5911 - val_loss: 1.6583 - val_accuracy: 0.7750 - 777ms/epoch - 16ms/step\n",
      "Epoch 15/15\n",
      "50/50 - 1s - loss: 1.6274 - accuracy: 0.6027 - val_loss: 1.7021 - val_accuracy: 0.7744 - 757ms/epoch - 15ms/step\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8512 - accuracy: 0.7595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7595000267028809"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN = Sequential()\n",
    "#Embedding層將「數字list」轉換成「向量list」\n",
    "modelCNN.add(Embedding(output_dim=32,   #輸出的維度是32，希望將數字list轉換為32維度的向量\n",
    "     input_dim=4000,                    #輸入的維度是4000，也就是我們之前建立的字典是4000字\n",
    "     input_length=100))                 #數字list截長補短後都是100個數字\n",
    "modelCNN.add(Conv1D(32, 2, activation=\"relu\", input_shape=(100,1)))\n",
    "modelCNN.add(MaxPooling1D())\n",
    "modelCNN.add(Dropout(0.3)) #隨機在神經網路中放棄30%的神經元，避免overfitting\n",
    "modelCNN.add(Conv1D(32, 2, activation=\"relu\"))\n",
    "modelCNN.add(MaxPooling1D())\n",
    "modelCNN.add(Dropout(0.3))\n",
    "\n",
    "modelCNN.add(Dense(16, activation=\"relu\"))\n",
    "modelCNN.add(Flatten())\n",
    "\n",
    "modelCNN.add(Dense(3, activation = 'sigmoid'))\n",
    "modelCNN.add(Dropout(0.3))\n",
    "modelCNN.compile(loss = 'sparse_categorical_crossentropy' ,optimizer = \"adam\",metrics = ['accuracy'])\n",
    "modelCNN.summary()\n",
    "\n",
    "train_history = modelCNN.fit(X_train, y_train,epochs=15, batch_size=128, verbose=2, validation_split=0.2)\n",
    "\n",
    "scores = modelCNN.evaluate(X_test, y_test,verbose=1)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ce8ffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 100, 32)           128000    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 100, 32)           0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               8448      \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145,025\n",
      "Trainable params: 145,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelLSTM = Sequential()\n",
    "modelLSTM.add(Embedding(output_dim = 32, input_dim = 4000, input_length = 100))\n",
    "modelLSTM.add(Dropout(0.2))\n",
    "modelLSTM.add(LSTM(32))\n",
    "modelLSTM.add(Dense(units = 256, activation = 'relu'))\n",
    "modelLSTM.add(Dropout(0.2))\n",
    "modelLSTM.add(Dense(1, activation ='sigmoid'))\n",
    "modelLSTM.compile(loss = 'binary_crossentropy' ,optimizer = \"adam\",metrics = ['accuracy'])\n",
    "modelLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fd30000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 7ms/step - loss: 0.6927 - accuracy: 0.6095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.609499990940094"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2 = modelLSTM.evaluate(X_test, y_test,verbose=1)\n",
    "scores2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110368c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
